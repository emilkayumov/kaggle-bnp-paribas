{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение конкурса \"BNP Paribas Cardif Claims Management\" на [kaggle.com](https://www.kaggle.com/c/bnp-paribas-cardif-claims-management).\n",
    "\n",
    "### \"Как успешно использовать чужие скрипты для построенния ансамбля\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Каюмов Эмиль, 317 группа ММП ВМК МГУ\n",
    "\n",
    "#### 42 место из 2947 участников (1 место среди студентов ММП ВМК МГУ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pwd = 'input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем заранее некоторые функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# добавление бинаризованных признаков\n",
    "def binarize(columnName, df, features=None):\n",
    "    df[columnName] = df[columnName].astype(str)\n",
    "    if features is None:\n",
    "        features = np.unique(df[columnName].values)\n",
    "    for x in features:\n",
    "        df[columnName+'_' + x] = df[columnName].map(lambda y: 1 if y == x else 0)\n",
    "    return df, features\n",
    "\n",
    "\n",
    "# добавление признака, полученного с помощью наивного байеса на категориальных признаках\n",
    "def add_bernulli(train, test, features):\n",
    "    for col in features:\n",
    "        train, binfeatures = binarize(col, train)\n",
    "        test, _ = binarize(col, test, binfeatures)\n",
    "        nb = BernoulliNB()\n",
    "        nb.fit(train[col + '_' + binfeatures].values, y_train)\n",
    "        train['naive_' + col] = nb.predict_proba(train[col + '_' + binfeatures].values)[:, 1]\n",
    "        test['naive_' + col] = nb.predict_proba(test[col + '_' + binfeatures].values)[:, 1]\n",
    "        train.drop(col + '_' + binfeatures, inplace=True, axis=1)\n",
    "        test.drop(col + '_' + binfeatures, inplace=True, axis=1)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# перевод буквенных значений признаков числовыми\n",
    "def factorize(train, test):\n",
    "    for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(), test.iteritems()):\n",
    "        if train_series.dtype == 'O':\n",
    "            train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "            test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        else:\n",
    "            tmp_len = len(train[train_series.isnull()])\n",
    "            if tmp_len > 0:\n",
    "                train.loc[train_series.isnull(), train_name] = -2\n",
    "            tmp_len = len(test[test_series.isnull()])\n",
    "            if tmp_len > 0:\n",
    "                test.loc[test_series.isnull(), test_name] = -2\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# вспомогательная функция для генерации метафич\n",
    "def stacking(X_train, X_test, y_train, skf, clfs):\n",
    "    meta_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "    meta_test  = np.zeros((X_test.shape[0],  len(clfs)))\n",
    "    \n",
    "    for j, clf in enumerate(clfs):\n",
    "        print('Clf', j+1)\n",
    "        meta_test_j = np.zeros((X_test.shape[0], len(skf)))\n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print('Fold', i+1)\n",
    "            X_tr = X_train[train]\n",
    "            y_tr = y_train[train]\n",
    "            X_ts = X_train[test]\n",
    "            y_ts = y_train[test]\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_submission = clf.predict_proba(X_ts)[:, 1]\n",
    "            meta_train[test, j] = y_submission\n",
    "            meta_test_j[:, i] = clf.predict_proba(X_test)[:, 1]\n",
    "        meta_test[:, j] = meta_test_j.mean(1)\n",
    "        gc.collect()\n",
    "        \n",
    "    return meta_train, meta_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс, реализующй добавление признаков, полученных с помощью линейной регрессией над несколькими признаками. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class addNearestNeighbourLinearFeatures:\n",
    "    \n",
    "    def __init__(self, n_neighbours=1, max_elts=None, verbose=True, random_state=None):\n",
    "        self.rnd = random_state\n",
    "        self.n = n_neighbours\n",
    "        self.max_elts = max_elts\n",
    "        self.verbose = verbose\n",
    "        self.neighbours = []\n",
    "        self.clfs = []\n",
    "        \n",
    "    def fit(self,train, y):\n",
    "        if self.rnd != None:\n",
    "            random.seed(rnd)\n",
    "        if self.max_elts == None:\n",
    "            self.max_elts = len(train.columns)\n",
    "        list_vars = list(train.columns)\n",
    "        random.shuffle(list_vars)\n",
    "        \n",
    "        lastscores = np.zeros(self.n) + 1e15\n",
    "\n",
    "        for elt in list_vars[:self.n]:\n",
    "            self.neighbours.append([elt])\n",
    "        list_vars = list_vars[self.n:]\n",
    "        \n",
    "        for elt in list_vars:\n",
    "            indice = 0\n",
    "            scores = []\n",
    "            for elt2 in self.neighbours:\n",
    "                if len(elt2) < self.max_elts:\n",
    "                    clf = LinearRegression(fit_intercept=False, normalize=True, copy_X=True, n_jobs=-1) \n",
    "                    clf.fit(train[elt2 + [elt]], y)\n",
    "                    scores.append(log_loss(y,clf.predict(train[elt2 + [elt]])))\n",
    "                    indice = indice + 1\n",
    "                else:\n",
    "                    scores.append(lastscores[indice])\n",
    "                    indice = indice + 1\n",
    "            gains = lastscores - scores\n",
    "            if gains.max() > 0:\n",
    "                temp = gains.argmax()\n",
    "                lastscores[temp] = scores[temp]\n",
    "                self.neighbours[temp].append(elt)\n",
    "\n",
    "        indice = 0\n",
    "        for elt in self.neighbours:\n",
    "            clf = LinearRegression(fit_intercept=False, normalize=True, copy_X=True, n_jobs=-1) \n",
    "            clf.fit(train[elt], y)\n",
    "            self.clfs.append(clf)\n",
    "            if self.verbose:\n",
    "                print(indice, lastscores[indice], elt)\n",
    "            indice = indice + 1\n",
    "                    \n",
    "    def transform(self, train):\n",
    "        indice = 0\n",
    "        for elt in self.neighbours:\n",
    "            train['_'.join(pd.Series(elt).sort_values().values)] = self.clfs[indice].predict(train[elt])\n",
    "            indice = indice + 1\n",
    "        return train\n",
    "    \n",
    "    def fit_transform(self, train, y):\n",
    "        self.fit(train, y)\n",
    "        return self.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для преобразования обучающей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(train, test):\n",
    "    \n",
    "    drop_columns = ['v8', 'v23', 'v25', 'v31', 'v36', 'v37', 'v46', 'v51', 'v53', 'v54', 'v63', 'v73', 'v75',\n",
    "                    'v79', 'v81', 'v82', 'v89', 'v92', 'v95', 'v105', 'v107', 'v108', 'v109', 'v110', 'v116', 'v117',\n",
    "                    'v118', 'v119', 'v123', 'v124', 'v128']\n",
    "    train.drop(drop_columns, axis=1, inplace=True)\n",
    "    test.drop(drop_columns, axis=1, inplace=True)\n",
    "    \n",
    "    naive_vars = ['v24', 'v112', 'v30', 'v91', 'v56', 'v74', 'v125', 'v71', 'v113', 'v47', 'v3', 'v66']\n",
    "    \n",
    "    train, test = factorize(train, test)\n",
    "    train, test = add_bernulli(train, test, naive_vars)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для ещё одного преобразования обучающей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(train, test):\n",
    "    trainids = train.ID.values\n",
    "    testids = test.ID.values\n",
    "    targets = train['target'].values\n",
    "    tokeep = [ 'v3', 'v10', 'v12', 'v14', 'v21', 'v22', 'v24', 'v30', 'v31', 'v34', 'v38', 'v40',\n",
    "              'v50', 'v52', 'v56', 'v62', 'v66', 'v71', 'v72', 'v74', 'v75', 'v79', 'v91', 'v47',  \n",
    "              'v112', 'v113', 'v114', 'v125', 'v129']\n",
    "    features = train.columns[2:]\n",
    "    todrop = list(set(features).difference(tokeep))\n",
    "    train.drop(todrop, inplace=True, axis=1)\n",
    "    test.drop(todrop, inplace=True, axis=1)\n",
    "    features = train.columns[2:]\n",
    "    for col in features:\n",
    "        if((train[col].dtype == 'object')):\n",
    "            train.loc[~train[col].isin(test[col]), col] = 'Orphans'\n",
    "            test.loc[~test[col].isin(train[col]), col] = 'Orphans'\n",
    "            train[col].fillna('Missing', inplace=True)\n",
    "            test[col].fillna('Missing', inplace=True)\n",
    "            train[col], tmp_indexer = pd.factorize(train[col])\n",
    "            test[col] = tmp_indexer.get_indexer(test[col])\n",
    "            traincounts = train[col].value_counts().reset_index()\n",
    "            traincounts.rename(columns={'index': col, col: col+'_count'}, inplace=True)\n",
    "            traincounts = traincounts[traincounts[col+'_count'] >= 50]\n",
    "            g = train[[col, 'target']].copy().groupby(col).mean().reset_index()\n",
    "            g = g[g[col].isin(traincounts[col])]\n",
    "            g.rename(columns={'target': col+'_avg'}, inplace=True)\n",
    "            train = train.merge(g, how='left', on=col)\n",
    "            test = test.merge(g, how='left', on=col)\n",
    "            h = train[[col, 'target']].copy().groupby(col).std().reset_index()\n",
    "            h = h[h[col].isin(traincounts[col])]\n",
    "            h.rename(columns={'target': col+'_std'}, inplace=True)\n",
    "            train = train.merge(h, how='left', on=col)\n",
    "            test = test.merge(h, how='left', on=col)\n",
    "            train.drop(col, inplace=True, axis=1)\n",
    "            test.drop(col, inplace=True, axis=1)\n",
    "\n",
    "    features = train.columns[2:]\n",
    "    train.fillna(-1, inplace=True)\n",
    "    test.fillna(-1, inplace=True)\n",
    "    train[features] = train[features].astype(float)\n",
    "    test[features] = test[features].astype(float)\n",
    "    ss = StandardScaler()\n",
    "    train[features] = np.round(ss.fit_transform(train[features].values), 6)\n",
    "    test[features] = np.round(ss.transform(test[features].values), 6)\n",
    "    gptrain = pd.DataFrame()\n",
    "    gptest = pd.DataFrame()\n",
    "    gptrain.insert(0, 'ID', trainids)\n",
    "    gptest.insert(0, 'ID', testids)\n",
    "    gptrain = pd.merge(gptrain, train[list(['ID'])+list(features)], on='ID')\n",
    "    gptest = pd.merge(gptest, test[list(['ID'])+list(features)], on='ID')\n",
    "    gptrain['TARGET'] = targets\n",
    "    del train\n",
    "    del test\n",
    "    gc.collect()\n",
    "    \n",
    "    return gptrain, gptest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся дальнейшая кросс-валидация по одним и тем же 10 фолдам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = train = pd.read_csv(pwd + 'train.csv')['target'].values\n",
    "cv = StratifiedKFold(y_train, n_folds=10, shuffle=True, random_state=115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "1. Удаление коррелированных и маловажных признаков, добавление признаков по наивному байесу над категориальными признаками (112 признаков).\n",
    "2. Только one-hot encoding над категориальными признаками с игнорированием значений признака v22, встречающихся менее 50 раз (около 600 признаков).\n",
    "3. Преобразование признаков с помощью линейной комбинацией над несколькими признаками (137 признаков).\n",
    "4. Ещё один датасет с оставлением малого числа признаков и фильтрацией значений (46 признаков)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking: 1 level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Организуем первый уровень стэкинга по 4 наборам данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = list(StratifiedKFold(y_train, n_folds=10, shuffle=True, random_state=115))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(pwd + \"train.csv\")\n",
    "test = pd.read_csv(pwd + \"test.csv\")\n",
    "\n",
    "y_train = train['target'].values\n",
    "id_test = test['ID'].values\n",
    "\n",
    "train.drop(['ID', 'target'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test = preprocess_data(train, test)\n",
    "\n",
    "X_train_n = np.array(X_train, dtype=np.float32)\n",
    "X_test_n = np.array(X_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем 3 алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=800, criterion='entropy', max_depth=37, max_features=25, \n",
    "                             min_samples_split=4, min_samples_leaf=2, n_jobs=-1, random_state=888),                  \n",
    "        XGBClassifier(n_estimators=600, learning_rate=0.03, max_depth=10, colsample_bytree=0.4, \n",
    "                      min_child_weight=1, seed=88888), \n",
    "        RandomForestClassifier(n_estimators=800, criterion='gini', max_depth=25, max_features=25, \n",
    "                               min_samples_split=1, min_samples_leaf=4, n_jobs=-1, random_state=777)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_1 = pd.DataFrame(meta_train, index=X_train.index, columns=['base_et', 'base_xgb', 'base_rf'])\n",
    "meta_test_1 = pd.DataFrame(meta_test, index=X_test.index, columns=['base_et', 'base_xgb', 'base_rf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тот же датасет, что и в предыдущем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_n = ss.fit_transform(X_train_n)\n",
    "X_test_n = ss.transform(X_test_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем только 1 алгоритм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = [LogisticRegression(C=1.0, penalty='l2', n_jobs=-1)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_2 = pd.DataFrame(meta_train, index=X_train.index, columns=['base_lr'])\n",
    "meta_test_2 = pd.DataFrame(meta_test, index=X_test.index, columns=['base_lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encodered features only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фильтруем v22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values, counts = np.unique(X_train.v22, return_counts=True)\n",
    "counts = {x : y for x, y in zip(values, counts)}\n",
    "X_train.v22 = X_train.v22.apply(lambda x: x if counts.get(x, 0) > 50 else 0)\n",
    "X_test.v22 = X_test.v22.apply(lambda x: x if counts.get(x, 0) > 50 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_vars = ['v24', 'v112', 'v30', 'v91', 'v52', 'v56', 'v74', 'v125', 'v71', 'v113', 'v47', 'v3', 'v66']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_vars += ['v22']\n",
    "X_train = X_train[cat_vars]\n",
    "X_test = X_test[cat_vars]\n",
    "gc.collect()\n",
    "\n",
    "data = pd.concat((X_train, X_test), axis=0, ignore_index=True)\n",
    "for col in cat_vars:\n",
    "    data = pd.concat((data, pd.get_dummies(data[col], prefix=col)), axis=1)\n",
    "    data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "X_train_n = np.array(data, dtype=np.float32)[:X_train.shape[0]]\n",
    "X_test_n = np.array(data, dtype=np.float32)[X_train.shape[0]:]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем 4 алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=45, max_features=30, \n",
    "                             min_samples_split=1, random_state=333, n_jobs=-1),\n",
    "        XGBClassifier(max_depth=8, learning_rate=0.05, n_estimators=600, min_child_weight=5, \n",
    "                      subsample=0.95, colsample_bytree=0.35, seed=901),\n",
    "        RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=50, max_features=20, \n",
    "                               min_samples_split=1, random_state=333, n_jobs=-1),\n",
    "        LogisticRegression(C=0.05, penalty='l2', n_jobs=-1)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_3 = pd.DataFrame(meta_train, index=X_train.index, columns=['ohe_et', 'ohe_xgb', 'ohe_rf', 'ohe_lr'])\n",
    "meta_test_3 = pd.DataFrame(meta_test, index=X_test.index, columns=['ohe_et', 'ohe_xgb', 'ohe_rf', 'ohe_lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Neighbors linear regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(pwd + \"train.csv\")\n",
    "test = pd.read_csv(pwd + \"test.csv\")\n",
    "\n",
    "train.drop(['ID', 'target'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "train, test = factorize(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['v22-1'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\n",
    "test['v22-1'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\n",
    "train['v22-2'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\n",
    "test['v22-2'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\n",
    "train['v22-3'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\n",
    "test['v22-3'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\n",
    "train['v22-4'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n",
    "test['v22-4'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n",
    "\n",
    "drop_list=['v91','v1', 'v8', 'v10', 'v15', 'v17', 'v25', 'v29', 'v34', 'v41', \n",
    "           'v46', 'v54', 'v64', 'v67', 'v97', 'v105', 'v111', 'v122']\n",
    "\n",
    "train = train.drop(drop_list,axis=1).fillna(-2)\n",
    "test = test.drop(drop_list,axis=1).fillna(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd = 12\n",
    "random.seed(rnd)\n",
    "n_ft = 20 # Number of features to add\n",
    "max_elts = 3 # Maximum size of a group of linear features\n",
    "\n",
    "a = addNearestNeighbourLinearFeatures(n_neighbours=n_ft, max_elts=max_elts, verbose=False, random_state=rnd)\n",
    "a.fit(train, y_train)\n",
    "\n",
    "train = a.transform(train)\n",
    "test = a.transform(test)\n",
    "\n",
    "X_train_n = np.array(train, dtype=float)\n",
    "X_test_n = np.array(test, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем 3 алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=31, max_features=50, \n",
    "                             min_samples_split=2, min_samples_leaf=2, random_state=610, n_jobs=-1),\n",
    "        XGBClassifier(n_estimators=350, learning_rate=0.05, max_depth=10, min_child_weight=5, \n",
    "                      subsample=1.0, colsample_bytree=0.4, seed=120),\n",
    "        RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=15, max_features=30, \n",
    "                               min_samples_split=3, min_samples_leaf=2, random_state=123, n_jobs=-1)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_4 = pd.DataFrame(meta_train, index=X_train.index, columns=['nnlr_et', 'nnlr_xgb', 'nnlr_rf'])\n",
    "meta_test_4 = pd.DataFrame(meta_test, index=X_test.index, columns=['nnlr_et', 'nnlr_xgb', 'nnlr_rf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another one script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(pwd + \"train.csv\")\n",
    "test = pd.read_csv(pwd + \"test.csv\")\n",
    "\n",
    "train, test = prepare_data(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.drop(['ID', 'TARGET'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "X_train_n = np.array(train, dtype=np.float32)\n",
    "X_test_n = np.array(test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Используем 3 алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=19, max_features=20, \n",
    "                             min_samples_split=2, min_samples_leaf=1, random_state=671, n_jobs=-1),\n",
    "        XGBClassifier(n_estimators=280, learning_rate=0.05, max_depth=8, min_child_weight=5, \n",
    "                      subsample=0.9, colsample_bytree=0.45, seed=511),\n",
    "        RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=12, max_features=15,\n",
    "                       min_samples_split=2, min_samples_leaf=1, random_state=181, n_jobs=-1)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_5 = pd.DataFrame(meta_train, index=X_train.index, columns=['small_et', 'small_xgb', 'small_rf'])\n",
    "meta_test_5 = pd.DataFrame(meta_test, index=X_test.index, columns=['small_et', 'small_xgb', 'small_rf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим полученные 14 метапризнаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train_level1 = pd.concat((meta_train_1, meta_train_2, meta_train_3, meta_train_4, meta_train_5), axis=1)\n",
    "meta_test_level1 = pd.concat((meta_test_1, meta_test_2, meta_test_3, meta_test_4, meta_test_5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_train_level1.to_csv(pwd + 'meta_train_level1.csv', index=False)\n",
    "meta_test_level1.to_csv(pwd + 'meta_test_level1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# meta_train_level1 = pd.read_csv(pwd + 'meta_train_level1.csv')\n",
    "# meta_test_level1 = pd.read_csv(pwd + 'meta_test_level1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking: 2 level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Организуем второй уровень стэкинга по 1, 3 и 4 наборам данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(pwd + \"train.csv\")\n",
    "test = pd.read_csv(pwd + \"test.csv\")\n",
    "\n",
    "y_train = train['target'].values\n",
    "id_test = test['ID'].values\n",
    "\n",
    "train.drop(['ID', 'target'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test = preprocess_data(train, test)\n",
    "\n",
    "X_train_2 = pd.concat((X_train, meta_train_level1), axis=1)\n",
    "X_test_2 = pd.concat((X_test, meta_test_level1), axis=1)\n",
    "\n",
    "X_train_n = np.array(X_train_2, dtype=np.float32)\n",
    "X_test_n = np.array(X_test_2, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем 2 алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=1000, max_features=25, criterion='entropy', min_samples_split=2, \n",
    "                             max_depth=36, min_samples_leaf=2, n_jobs=-1, random_state=888),\n",
    "        XGBClassifier(n_estimators=600, learning_rate=0.03, max_depth=10, colsample_bytree=0.4, \n",
    "                      min_child_weight=1, seed=129)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_0 = pd.DataFrame(meta_train, index=X_train.index, columns=['main_et', 'main_xgb'])\n",
    "meta_test_0 = pd.DataFrame(meta_test, index=X_test.index, columns=['main_et', 'main_xgb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Neighbors linear regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(pwd + \"train.csv\")\n",
    "test = pd.read_csv(pwd + \"test.csv\")\n",
    "\n",
    "train.drop(['ID', 'target'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "train, test = factorize(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['v22-1'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\n",
    "test['v22-1'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\n",
    "train['v22-2'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\n",
    "test['v22-2'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\n",
    "train['v22-3'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\n",
    "test['v22-3'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\n",
    "train['v22-4'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n",
    "test['v22-4'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n",
    "\n",
    "drop_list=['v91','v1', 'v8', 'v10', 'v15', 'v17', 'v25', 'v29', 'v34', 'v41', \n",
    "           'v46', 'v54', 'v64', 'v67', 'v97', 'v105', 'v111', 'v122']\n",
    "\n",
    "train = train.drop(drop_list,axis=1).fillna(-2)\n",
    "test = test.drop(drop_list,axis=1).fillna(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnd = 12\n",
    "random.seed(rnd)\n",
    "n_ft = 20 # Number of features to add\n",
    "max_elts = 3 # Maximum size of a group of linear features\n",
    "\n",
    "a = addNearestNeighbourLinearFeatures(n_neighbours=n_ft, max_elts=max_elts, verbose=False, random_state=rnd)\n",
    "a.fit(train, y_train)\n",
    "\n",
    "train = a.transform(train)\n",
    "test = a.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_2 = pd.concat((train, meta_train_level1), axis=1)\n",
    "X_test_2 = pd.concat((test, meta_test_level1), axis=1)\n",
    "\n",
    "X_train_n = np.array(X_train_2, dtype=np.float32)\n",
    "X_test_n = np.array(X_test_2, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем 2 алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=32, max_features=50, \n",
    "                             min_samples_split=2, min_samples_leaf=2, random_state=171, n_jobs=-1),\n",
    "        XGBClassifier(n_estimators=330, learning_rate=0.03, max_depth=8, colsample_bytree=0.45, \n",
    "                      min_child_weight=5, subsample=1.0, seed=666)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_00 = pd.DataFrame(meta_train, index=train.index, columns=['main_nnlr_et', 'main_nnlr_xgb'])\n",
    "meta_test_00 = pd.DataFrame(meta_test, index=test.index, columns=['main_nnlr_et', 'main_nnlr_xgb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another one script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(pwd + \"train.csv\")\n",
    "test = pd.read_csv(pwd + \"test.csv\")\n",
    "\n",
    "train, test = prepare_data(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.drop(['ID', 'TARGET'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_2 = pd.concat((train, meta_train_level1), axis=1)\n",
    "X_test_2 = pd.concat((test, meta_test_level1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_n = np.array(X_train_2, dtype=np.float32)\n",
    "X_test_n = np.array(X_test_2, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем 2 алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=14, max_features=32, \n",
    "                             min_samples_split=2, min_samples_leaf=3, random_state=10, n_jobs=-1),\n",
    "        XGBClassifier(n_estimators=650, learning_rate=0.01, max_depth=7, colsample_bytree=0.4,\n",
    "              min_child_weight=5, subsample=1.0, seed=89)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_000 = pd.DataFrame(meta_train, index=train.index, columns=['main_small_et', 'main_small_xgb'])\n",
    "meta_test_000 = pd.DataFrame(meta_test, index=test.index, columns=['main_small_et', 'main_small_xgb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим полученные 6 метапризнаков второго уровня."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_train_level2 = pd.concat((meta_train_0, meta_train_00, meta_train_000), axis=1)\n",
    "meta_test_level2 = pd.concat((meta_test_0, meta_test_00, meta_test_000), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train_level2.to_csv(pwd + 'meta_train_level2.csv', index=False)\n",
    "meta_test_level2.to_csv(pwd + 'meta_test_level2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# meta_train_level2 = pd.read_csv(pwd + 'meta_train_level2.csv')\n",
    "# meta_test_level2 = pd.read_csv(pwd + 'meta_test_level2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем четвёртый датасет с наименьшим количеством признаков, прибавим 20 метапризнаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(pwd + \"train.csv\")\n",
    "test = pd.read_csv(pwd + \"test.csv\")\n",
    "\n",
    "X_train, X_test = prepare_data(train, test)\n",
    "\n",
    "X_train.drop(['ID', 'TARGET'], axis=1, inplace=True)\n",
    "X_test.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_3 = pd.concat((X_train, meta_train_level1, meta_train_level2), axis=1)\n",
    "X_test_3 = pd.concat((X_test, meta_test_level1, meta_test_level2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем калибровку полученных вероятностей по тем же фолдам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = CalibratedClassifierCV(ExtraTreesClassifier(n_estimators=2000, max_features=30, criterion='entropy', \n",
    "                                                  min_samples_split=2, max_depth=14, min_samples_leaf=2, \n",
    "                                                  n_jobs=-1, random_state=190), method='isotonic', cv=cv)\n",
    "\n",
    "clf.fit(X_train_3, y_train) \n",
    "y_pred = clf.predict_proba(X_test_3)[:, 1]\n",
    "\n",
    "pd.DataFrame({\"ID\": id_test, \"PredictedProb\": y_pred}).to_csv('68_18042016.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
